{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from umap.umap_ import UMAP\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine the model architecture but instead of returning the last layer, we return the penultimate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            nn.Conv2d(3,9,(5,5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(9,18,(5,5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Flatten(1,-1),\n",
    "            nn.Linear(288,64)\n",
    "            )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l = nn.Linear(64,5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/hglmht2s7y19n14x7s3vfm2m0000gn/T/ipykernel_59707/3602087002.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_images = torch.load('Saved Data/Data/train_images.pt')\n",
      "/var/folders/bn/hglmht2s7y19n14x7s3vfm2m0000gn/T/ipykernel_59707/3602087002.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_labels = torch.load('Saved Data/Data/train_labels.pt')\n",
      "/var/folders/bn/hglmht2s7y19n14x7s3vfm2m0000gn/T/ipykernel_59707/3602087002.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_images = torch.load('Saved Data/Data/test_images.pt')\n",
      "/var/folders/bn/hglmht2s7y19n14x7s3vfm2m0000gn/T/ipykernel_59707/3602087002.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_labels = torch.load('Saved Data/Data/test_labels.pt')\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "model.load_state_dict(torch.load('Saved Data/models/model.pt', weights_only=True))\n",
    "model.eval()\n",
    "train_images = torch.load('Saved Data/Data/train_images.pt')\n",
    "train_labels = torch.load('Saved Data/Data/train_labels.pt')\n",
    "test_images = torch.load('Saved Data/Data/test_images.pt')\n",
    "test_labels = torch.load('Saved Data/Data/test_labels.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the training data was not corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 3, 28, 28])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort all the images by labels, and keep track of the index for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = T.Compose([\n",
    "    T.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "])\n",
    "train_output = [[] for _ in range(5)]\n",
    "train_indices = [[] for _ in range(5)]\n",
    "test_output = [[] for _ in range(5)]\n",
    "test_indices = [[] for _ in range(5)]\n",
    "with torch.no_grad():\n",
    "    for index,(x, label) in enumerate(zip(train_images, train_labels)):\n",
    "        train_output[label].append(model(transform(torch.unsqueeze(x,0)))[0])\n",
    "        train_indices[label].append(index)\n",
    "    for index, (x, label) in enumerate(zip(test_images, test_labels)):\n",
    "        test_output[label].append(model(transform(torch.unsqueeze(x,0)))[0])\n",
    "        test_indices[label].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  5.6822,   0.7227,  -3.3618,  -2.8614,  14.4085,  22.6241,  -2.7060,\n",
      "         -4.6660,  -4.1816,  -2.0434,   3.4939,  -2.3240,  -2.4109,  -6.6759,\n",
      "          9.2076,   9.0879,  -6.5901,  -1.6390,  -3.6716,   8.9184,   8.5940,\n",
      "         15.7716,   0.0287,  -6.3466,   2.6248,   0.9239,   1.3071,   0.1489,\n",
      "         11.7156, -11.5195,  -1.1184,  10.9707,  -2.9344,  -7.8740,   8.0705,\n",
      "         -2.9119,   4.5426,  -2.0824,   5.6963,  10.0487,   9.3382, -14.4961,\n",
      "          9.7506,  -2.6642,   6.8131,   0.8836,   6.7535,  -4.0043,  -1.6445,\n",
      "          5.5787,  -2.2131, -10.3292,   6.1588,  -1.7774,   3.7803,   1.2636,\n",
      "         -2.3724,   4.2579,   4.4073,   1.5223,  -3.9662,  10.7345,   5.3141,\n",
      "         -2.8403])\n"
     ]
    }
   ],
   "source": [
    "print(train_output[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a reducer to transform n-dimensional tensors in 2-dimensional tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer1 = UMAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data to aid with dimensionality reducing speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [StandardScaler().fit_transform(train_output[i]) for i in range(5)]\n",
    "test = [StandardScaler().fit_transform(test_output[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dimensionality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10133, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = [reducer1.fit_transform(train[i]) for i in range(5)]\n",
    "test = [reducer1.fit_transform(test[i]) for i in range(5)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every single group of labels, figure out the number of clusters and their centers by trying out 2-10 clusters and seeing which one results in the highest silhouette score. Then, for each cluster that we found, break it into F subcluster, F in this case being 10. Record all the centers of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bk = [0,0,0,0,0]\n",
    "f = 10\n",
    "centers = []\n",
    "for i in range(5):\n",
    "    bs = 0\n",
    "    for k in range(2,10):\n",
    "        gm = GaussianMixture(k).fit_predict(e[i])\n",
    "        score = silhouette_score(train[i], gm)\n",
    "        if score > bs:\n",
    "            bs = score\n",
    "            bk[i] = k\n",
    "    predictor = GaussianMixture(bk[i]).fit(train[i])\n",
    "    temp = []\n",
    "    labels = predictor.predict(train[i])\n",
    "    for cluster_value in np.unique(labels):\n",
    "        cluster = train[i][labels == cluster_value]\n",
    "        subclusters = GaussianMixture(f).fit(cluster).means_\n",
    "        for subsubclusters in subclusters:\n",
    "            temp.append(subsubclusters)\n",
    "    centers.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label each image according to which subcluster it belongs to. We add num to make sure that images that belong to different labels dont have the same subcluster number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([])\n",
    "test_labels = np.array([])\n",
    "num = 0 #just making sure that each group has a different value\n",
    "for i in range(5):\n",
    "    train_labels = np.concatenate((train_labels, KMeans(len(centers[i]), init = centers[i]).fit_predict(train[i])  + num))\n",
    "    test_labels  = np.concatenate(( test_labels, KMeans(len(centers[i]), init = centers[i]).fit_predict(test[i]) + num))\n",
    "    num += len(centers[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the array of subcluster labels in order using the indicies we recorded earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_i = np.array([])\n",
    "test_i = np.array([])\n",
    "for i in range(5):\n",
    "    train_i = np.concatenate((train_i, train_indices[i]))\n",
    "    test_i  = np.concatenate((test_i, test_indices[i]))\n",
    "\n",
    "\n",
    "train_idx = np.argsort(train_i)\n",
    "train_labels = np.array(train_labels)[train_idx]\n",
    "train_index = np.array(train_i)[train_idx]\n",
    "\n",
    "test_idx = np.argsort(test_i)\n",
    "test_labels = np.array(test_labels)[test_idx]\n",
    "test_index = np.array(test_i)[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'Labels': train_labels})\n",
    "df_test = pd.DataFrame({'Labels' : test_labels})\n",
    "df_train.to_csv('Saved Data/Groups/training_groups.csv', index= False)\n",
    "df_test.to_csv('Saved Data/Groups/testing_groups.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
